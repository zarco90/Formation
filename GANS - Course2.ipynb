{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course 2 Build Better Generative Adversarial Networks (GANs)\n",
    "\n",
    "## W02 - Week 2: GAN Disadvantages and Bias\n",
    "\n",
    "### Disadvantages of GANs\n",
    "\n",
    "GANs also have their disadvantages:\n",
    "* They lack concrete theoretically grounded intrinsic evaluation metrics. In order to evaluate your GAN, you might remember that you usually need to inspect the features across many generated samples and compare them to those of the real images, and even that technique isn't that reliable, it's an approximate estimate of what you would ideally want for your evaluation.\n",
    "\n",
    "* During training, the model can be unstable and take considerable amount of time to train. At the same time, you've also seen this problem being remedied with W loss a bit and one Lipschitz continuity. While this is an issue, it's not necessarily a huge one anymore.\n",
    "\n",
    "* GANs might not be the right type of model if you want to explicitly get the probability density over your modeled features This is known as density estimation because it's estimating this probability density of all these features.\n",
    "\n",
    "* The generator is not trained to be invertible. What that means is that you can take an image such as this one, and be able to figure out what noise vector it maps onto, so the opposite task. Inversion can be particularly convenient for image editing because that means you can apply your controllable generation skills to that noise vector that you find for any image, and this could be a real image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative to GANs\n",
    "\n",
    "A generative model can be any machine learning model that tries to model this P of X given Y. Or if it's really just modeling that one class it's probably P of X of that data, and often it'll take in some kind of noise for that stochastic city so that you don't generate the same thing each time.\n",
    "\n",
    "Variational autoencoders or VAEs is another large family of generative models. VAEs work with two different models an encoder in a decoder. And it learns by feeding real images into that encoder, finding a good way of representing that image in this latent space perhaps here. And then taking that link representation reconstructing the realistic image the encoder saw before with this decoder.\n",
    "At a high level VAEs tries to minimize the divergent between the generated and the real distributions. After, if you train the VAE you actually loop off the encoder just like you don't need the discriminator and you use the decoder similar to the generator.\n",
    "\n",
    "So if you remember the pros and cons of GANs, VAEs are more or less flipped. So VAEs typically have been seen as producing lower quality results than GANs, or not the first and reducing realistic results. There are definitely behind GANs required a little bit more engineering and changes, but they have density estimation. They can invert easily because they have that encoder to try to find that latent space representation. And training is also more stable and reliable, though arguably it's fairly slow.\n",
    "\n",
    "An autoregressive model is a model that looks at previous pixels to determine the next pixel. It can't see into future pixels. And as you can probably tell this model is not fully unsupervised because it depends on those previous pixels. So it is a supervised technique, meaning it will require anchor pixels to start generating, can't generate from noise.\n",
    "\n",
    "Another type of generative model is a flow model, and these are hard and long to train. That's based on likelihood defined in invertible mapping between the noise in the generated image.  What it's doing is from an initial simple distribution it finds sequences of invertible transformations to create more complex distributions.\n",
    "\n",
    "Finally, you can also combine any of these models or ideas to form hybrid architectures to try to reap the benefits of two or more worlds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fairness definition\n",
    "\n",
    "Fairness Definitions Explained (Verma and Rubin, 2018): https://fairware.cs.umass.edu/papers/Verma.pdf\n",
    "\n",
    "A Survey on Bias and Fairness in Machine Learning (Mehrabi, Morstatter, Saxena, Lerman, and Galstyan, 2019): https://arxiv.org/abs/1908.09635\n",
    "\n",
    "It can be defined as having equalized odds that the outcome is independent of a sensitive attribute such as ethnicity, and this is also known as demographic parity.\n",
    "\n",
    "All else being equal, the probability that you predict correctly or incorrectly is the same for different values of a protected class, also known as equality of odds.\n",
    "\n",
    "Does Object Recognition Work for Everyone? (DeVries, Misra, Wang, and van der Maaten, 2019): https://arxiv.org/abs/1906.02659\n",
    "\n",
    "What a machine learning tool that turns Obama white can (and can't) tell us about AI bias (Vincent, 2020): https://www.theverge.com/21298762/face-depixelizer-ai-machine-learning-tool-pulse-stylegan-obama-bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ways bias is introduced\n",
    "\n",
    "So one way bias can enter your model is during the training phase. So first you should pay attention to your training data set. And this could mean there could be no variation in the data, or the data might express biases that come from how they were collected. And you also need to consider how your data was collected, was the data all collected from one location, one web scrape? Was it collected by a single person, or a single demographic of people.\n",
    "\n",
    "Biases that exist in broader society can also shape all portions of your model. Evaluation methods could be biased towards images that are often regarded as quote, Unquote. Good or correct in that society or in one culture, but not another.\n",
    "\n",
    "More than half of the images in ImageNet, come from the USA and Great Britain. Compared to the population densities of the world, this is not really representative of where most people are from. And that imbalance leads systems to inaccurately classify images into categories that differ by geography.\n",
    "\n",
    "The way evaluation calculations are computed, can reinforce biases within the model you develop. And can make you think a model is great at a certain task, when it actually is unable to perform that task.\n",
    "\n",
    "Bias can be introduced through the architecture as well. What was the diversity of programmers who optimize the code? It's even more important to lend a critical eye, to how various problems are chosen in this field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W03 - Build Better Generative Adversarial Networks (GANs)\n",
    "\n",
    "## GAN improvements\n",
    "\n",
    "The first major improvement was to GAN training stability. The standard deviation of these images can show when your generator is approaching mode collapse. To mitigate this, perhaps you can pass this information to the discriminator and it can then punish the generator.\n",
    "\n",
    "Another way researchers have improved stability in GANs is by enforcing one Lipschitz continuity when using Wasserstein loss or W loss. This prevents a model from both burning too fast and also keeping in valid W distance, where you want to make sure the gradient of your function remains in these green triangles at every single point of this function.\n",
    "\n",
    "Another method to enforce this is also to use Spectral Normalization on the weights. Another method is called Spectral Normalization, which in some senses they are similar to Batch Normalization, in the sense that it normalizes the weights and it's often instantiated as some layer.\n",
    "\n",
    "A final way researchers have tried to stabilize GANs is by using two techniques.\n",
    "The first is using a Moving Average that takes the average weights over several checkpoints. Instead of describing the weights of one generator, which would be typically Theta_i. Instead you want the average Theta, which will equal the sum of j to n Theta_j. Instead you want the average of all of these parameters. This gives you much smoother results and is done during training, but also can be applied when saving a model and then sampling from it.\n",
    "\n",
    "Without averaging, you can see that maybe that generator checkpoint was in some local minimum or it just wasn't very smooth versus using some averaging. The exponential average can yield much smoother result and the differences are quite apparent.\n",
    "\n",
    "The second technique is called Progressive Growing. It gradually trains your generator on increasing image resolutions. It can generate high-quality images.\n",
    "\n",
    "Another major improvement to GANs was their capacity. Generators and discriminators have gone a lot wider and deeper, such as deep convolutional GANs. These models can be larger because of improved hardware like better GPUs in higher resolution data sets, like the FFHQ data set, which is a set of face images from Flickr that are really high resolution to a trained StyleGAN, the state of the StyleGAN.\n",
    "\n",
    "The final major improvement to GANs has also been their diversity and their output. One of the ways diversity has been improved is by having larger data sets with more coverage because having more variety in the images being trained on, lead to the same in the output as well. The method using mini-batch standard deviation to perhaps help with the stability and training can also help with increasing and improving diversity by avoiding more collapse and turning towards how much variation real images have. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StyleGAN overview\n",
    "\n",
    "StyleGAN is a relatively new architecture that's considered to not only be the state of the art GAN right now, but an inflection point improvement in GAN, particularly in its ability to generate extremely realistic human faces.\n",
    "\n",
    "One of the first goals of StyleGAN is to produce high quality, high resolution images that are capable of pulling a casual onlooker. The second is a greater diversity of images in the output.\n",
    "\n",
    "One of the really cool things about StyleGAN is also the increased control over image features. And this can be adding features like hats or sunglasses or mixing styles from two different generated images together.\n",
    "\n",
    "Getting high resolution fidelity that fools the untrained eye is a huge feat that up until recently, was much more difficult to achieve. And this was mainly due to smaller model capacity, lower resolution datasets, and also the challenge of high resolution just really wasn't tackled until ProGAN, which is StyleGAN predecessor that was launched in 2018.\n",
    "\n",
    "Finally, StyleGAN also wants to increase the control you have over your image feature. And this can be mixing the style of 1 image into another. Control could also mean adding accessories such as glasses and style. StyleGAN accomplishes this by disentangling the latent space which you'll be learning about in more detail here shortly.\n",
    "\n",
    "The word style in the context of image generation is pretty much any variation in the image. And you can think of these variations as generally representing the look and feel of different levels in the image. And these different levels could mean larger, coarser styles such as general face shape or facial structure to finer, more detailed styles which are like color their hair or placement of certain wisps of hair. The StyleGAN generator, Interestingly enough, is made up of blocks where earlier blocks roughly aligned with coarser features, like facial structure or pose. So the earlier layers here and nearer to the output, finer details styles such as hair color, eyebrow shape, are controlled.\n",
    "\n",
    "In a traditional GAN generator, you taken this noise vector into the generator and the generator then outputs an image. Now in StyleGAN, the noise vector is handled a little bit differently, it goes through a mapping network to get an intermediate noise vector W, that then gets injected into the StyleGAN generator, actually multiple times to produce the image. And also there's this extra random noise that's passed in to add various stochastic variation onto this image.\n",
    "\n",
    "W is not just inputed easily into the StyleGAN generator. Instead, styles are extracted from the W value and then added to various points in the StyleGAN generator. And an earlier points of the StyleGAN, generator W will inform course or styles that you saw previously and injecting W into layer layers will control styles of finer grained things. And this injection of this intermediate noise into all of these layers of StyleGAN is done through an operation called AdaIN or Adaptive Instance Normalization. And this is the type of normalization similar to batch normalization, except that after it normalizes in some way, it tries to apply some kind of style which is just statistics of an image based on this W coming in.\n",
    "\n",
    "The third and final important component of StyleGAN is progressive growing, which slowly grows the image resolution being generated by the generator and evaluated by the discriminator over the process of training. So during the training process, both the generator and discriminator start with a small low resolution image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progressive growing\n",
    "\n",
    "Progressive growing is one of the components of StyleGAN.\n",
    "\n",
    "Progressive growing and StyleGAN is trying to make it so it's easier for the generator to generate higher resolution images by gradually training it from lower resolution images to those higher resolution images. Starting with an easier task of say, a very blurry image for it to generate a four by four image with only 16 pixels to then a much higher resolution image overtime. Finally, over time, over and perhaps lots of different growing periods and these are scheduled intervals during training, the generator will be able to generate super high-resolution images and the discriminator will look at that higher resolution image against real images that will also be at this high resolution, so no longer downsampled and be able to detect whether it's real or fake.\n",
    "\n",
    "This progressive growing isn't as straightforward as just doubling in size immediately at this scheduled intervals, it's actually a little bit more gradual than that. You can think of this as an alpha parameter that grows over time where alpha starts out as 0 and this is just completely one. Initially, the upsampling is made by some method like KNN with no learned parameters and progresively with decreasing alpha, an upsampling learning method is having more weight.\n",
    "<img src=\"images/gan_upsampling.PNG\"  style=\"width: 600px;\">\n",
    "\n",
    "For the discriminator, there's something fairly similar, but in the opposite direction. Slowly over time, you go through this downsampling layer. The discriminator takes in the image instead of generating the image at the very end, gives you just one output of real or fake and essentially a probability of between zero and one.\n",
    "<img src=\"images/gan_upsampling_disc.PNG\"  style=\"width: 600px;\">\n",
    "\n",
    "Progressive growing in the context of StyleGAN essentially is a series of all of these blocks that progressively grow into higher and higher resolution outputs and the StyleGAN generator essentially will look like this. Each of these blocks will be composed of an upsampling layer as well as a convolutional layer and actually they will have each two convolutional layers to be able to learn even more.\n",
    "<img src=\"images/gan_upsampling_sum.PNG\"  style=\"width: 400px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise mapping network\n",
    "\n",
    "Adding the noise vector as input will help control styles later on. The noise mapping network actually takes your noise vector Z and maps it into an intermediate noise vector W. And this noise mapping network is composed of eight fully connected layers with activations in between, also known as a multilayer perceptron or MLP. It's a simple neural network that takes your Z noise vector, and maps it onto your W intermediate noise factor, so it just changes the values but no noise vector dimensions.\n",
    "<img src=\"images/gan_nmp_1.PNG\" alt=\"NMP\" style=\"width: 500px;\"/>\n",
    "\n",
    "The motivation behind this is that mapping your noise vector will actually get you a more disentangled representation. Having this intermediate mapping for the noise vector can allow for that space, and now this is called W-Space.\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"images/gan_nmp_2.PNG\" alt=\"Entangled\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"images/gan_nmp_3.PNG\" alt=\"Disentangled\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "To match the real data's density and this makes it easier to learn a one-to-one mapping, that's disentangled. W it's just less entangled than Z-Space.\n",
    "\n",
    "And this is what the generator architecture kind of looks like. And the noise mapping network actually comes in here as input into all of these blocks. W actually is inputed at multiple different places along the network. Its influence differs where it's inputed earlier versus later. W doesn't actually go in the beginning of the network unlike what Z did in the original GAN set up. Instead of inputting W at the very beginning, what the model starts with is actually just a constant value. Basically just a fixed value that doesn't change for any image you generate. This is of size 4 x 4 x 512. Any changes to any image being generated will occur where the noise vector is being introduced, which is everywhere.\n",
    "\n",
    "<img src=\"images/gan_nmp_4.PNG\" alt=\"W into NMP\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Instance Normalization (AdaIN)\n",
    "\n",
    "Adaptive instance normalization are what transfers style information from the intermediate noise vector w onto your generated image. Instance normalization essentially normalize each instance, while the adaptive part in adaptive instance normalization is able to apply the different styles from the intermediate noise vector w onto that image or onto that intermediate feature map. AdaIN actually comes in after each convolutional layer.\n",
    "<table><tr>\n",
    "<td> <img src=\"images/gan_AdaIN_1.PNG\" alt=\"AdaIN in context\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"images/gan_AdaIN_2.PNG\" alt=\"Normalization\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "It's actually not based on the batch necessarily. In batch norm you look across the height and width of the image. You look at a channel, and you look at all N examples in the mini batch. And then, you get the mean and standard deviation based on all of these highlighted blue cells for one channel in one batch and for all the batches and for all the channels. Instance normalization is a little bit different. Instance normalization, only looks at one example or one instance, so an example is also known as an instance. So it doesn't look across statistics of the entire batch, it only looks at one example, and again only one channel of that example. Just getting the statistics from just that one channel, one instance, normalizing those values in there based on its mean and it standard deviation. So to represent that in the equation, we actually call an instance i. Eevery value in that instance will be normalized to a mean of 0 and a standard deviation of 1. That's the first step of adaptive instance normalization, the instance normalization part.\n",
    "\n",
    "<img src=\"images/gan_AdaIN_3.PNG\" style=\"width: 500px;\"/>\n",
    "\n",
    "Where the adaptive part comes in is to apply adaptive styles to this now normalized set of values. The adaptive styles are coming from your intermediate noise vector W. W inform styles which are then imported into AdaIN. It goes through learned parameters, such as two fully connected layers, and produces two parameters for us. One is ys that stands for scale, and the other is yb, which stands for bias. These statistics are then imported into the AdaIN layers. Then you want to reshift and rescale your values based on these statistics that are extracted from the intermediate noise factor. So the second step is getting these adaptive styles. Essentially this middle instance normalization part is trying to undo any style related information that was originally there.\n",
    "\n",
    "<img src=\"images/gan_AdaIN_4.PNG\" style=\"width: 500px;\"/>\n",
    "\n",
    "AdaIN is used at every single block here in the generator to essentially get the style information from w into those feature maps. So W is added into all of these blocks, and when it's added in into these earlier blocks, it'll be those coarser details that are changed or affected by this w style. Whereas in these later blocks, it'll be finer details that are informed by W. As this occurs at every single one of these blocks, this means that every block will control styles at that block. And at the next block, it will be overwritten by the next AdaIN in the normalizes those previous outputs, and that even happens within the block you see here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Style and Stochastic Variation\n",
    "\n",
    "Style mixing, which mixes the different intermediate noise vectors. Stochastic noise adds a little bit more extra noise into your model and your images. You can control coarse and fine styles with StyleGAN, using two different methods. The first is style mixing for increased diversity during training and inference, and this is mixing two different noise vectors that get inputted into the model. The second is adding stochastic noise for additional variation in your images. \n",
    "\n",
    "Although W is injected in multiple places in the network, it doesn't actually have to be the same W each time. You can actually have multiple W's. You can actually sample a Z, let's say Z_1. You're sampling a Z that goes through the mapping network, you get a W, it's associated W_1, and you injected that into, the first half of the network. Then you sample another Z, Z_2, and that gets you W_2, and then you put that in to the second half of the network. You inject that into all the different blocks in the second half of the network. The switch off between W_1 and W_2 can actually be at any point really, it doesn't have to be exactly the middle for half and half the network. This can help you control what variation you'd like. The later the switch, the finer the features that you'll get from W_2. W_2 his largely informing these finer control features, and W1 was originally controlling these top more coarse features. As you might expect, this improves your diversity as well since your model is trained like this, so that is constantly mixing different styles and it can get more diverse outputs. You can input different W_1, W_2 vectors, and you can get these mixes and you can control the degree to which how much you want of one image versus another, and what type of styles coarse, middle or fine, that you want from each of these intermediate vectors.\n",
    "<table><tr>\n",
    "<td> <img src=\"images/gan_SaSV_1.PNG\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"images/gan_SaSV_2.PNG\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>\n",
    "StyleGAN can perform smaller variations that don't require to mix two different images, which is adding this additional noise to the model, which will add stochastic variation to your image. You can inject this random noise into the finer layers or the coarse layers and will get different effects on the final output.\n",
    "In order to do that, is actually a separate process of injecting noise, so it has nothing to do with Z or W here. This noise will actually be added in before your adaptive instance normalization. But first you want to sample noise from a normal distribution, so you just sample completely random values from this, and then those noise values are then concatenated to your X, which is your convolutional feature map output before it goes into adoptive instance normalization AdaIN. That's just adding some stochasticity, some randomness into your image, into those values. The degree to which this noise actually affects these convolutional outputs is controlled by a factor, let's call it Lambda 1 here and Lambda 2. These are learned value.\n",
    "\n",
    "<img src=\"images/gan_SaSV_3.PNG\" style=\"width: 500px;\"/>\n",
    "\n",
    "To recap style mixing with your intermediate noise vectors, can increase the diversity that the model sees during training and allow you to control coarse or finer styles. Stochastic noise is another way to inject variation into your output. Also the coarse or fineness depends on where in the network your style mixing or noise is added in earlier for coarser variation and later for finer variation, which is pretty consistent across neural networks, including classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    "* Generative Adversarial Networks (Goodfellow et al., 2014): https://arxiv.org/abs/1406.2661\n",
    "* Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (Radford, Metz, and Chintala, 2016): https://arxiv.org/abs/1511.06434\n",
    "* Coupled Generative Adversarial Networks (Liu and Tuzel, 2016): https://arxiv.org/abs/1606.07536\n",
    "* Progressive Growing of GANs for Improved Quality, Stability, and Variation (Karras, Aila, Laine, and Lehtinen, 2018): https://arxiv.org/abs/1710.10196\n",
    "* A Style-Based Generator Architecture for Generative Adversarial Networks (Karras, Laine, and Aila, 2019): https://arxiv.org/abs/1812.04948\n",
    "* The Unusual Effectiveness of Averaging in GAN Training (Yazici et al., 2019): https://arxiv.org/abs/1806.04498v2\n",
    "* Progressive Growing of GANs for Improved Quality, Stability, and Variation (Karras, Aila, Laine, and Lehtinen, 2018): https://arxiv.org/abs/1710.10196\n",
    "* StyleGAN - Official TensorFlow Implementation (Karras et al., 2019): https://github.com/NVlabs/stylegan\n",
    "* StyleGAN Faces Training (Branwen, 2019): https://www.gwern.net/images/gan/2019-03-16-stylegan-facestraining.mp4\n",
    "* Facebook AI Proposes Group Normalization Alternative to Batch Normalization (Peng, 2018): https://medium.com/syncedreview/facebook-ai-proposes-group-normalization-alternative-to-batch-normalization-fb0699bffae7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
